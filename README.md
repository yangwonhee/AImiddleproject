# T5

# 1. Introduction

ğŸ”—Â ë…¼ë¬¸ ë§í¬

[Exploring the Limits of Transfer Learning with a Unified...](https://arxiv.org/abs/1910.10683)

![Untitled](img/Untitled.png)

## T5ì˜ íŠ¹ì§•

1. text-to-text framework

![Untitled](img/Untitled%201.png)

Text-to-TextëŠ” ë³´í†µ generation taskì—ì„œ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ì¸ë°, T5ì—ì„œëŠ” generationë¿ë§Œ ì•„ë‹ˆë¼ classification, regression ë¬¸ì œë„ Text-to-Textë¡œ í’€ë ¤ê³  í–ˆë‹¤. ì´ë ‡ê²Œ ëª¨ë“  taskë“¤ì„ í•˜ë‚˜ì˜ ì ‘ê·¼ ë°©ë²•ìœ¼ë¡œ í’€ê²Œ ëœë‹¤ë©´ **ë‹¤ì–‘í•œ** downstream taskì—Â **ë™ì¼í•œ**Â model, objective, training procedure, decoding processë¥¼ ì ìš©í•  ìˆ˜ ìˆê²Œ ëœë‹¤.

T5ëŠ” íšì¼í™”ëœ ë°©ì‹ì„ í†µí•´ ë‹¤ì–‘í•œ transfer learning objectiveì™€ unlabeled dataset ê°™ì€ ë‹¤ì–‘í•œ ëª¨ë¸ë§ ìš”ì†Œë“¤ì— ëŒ€í•´ì„œ íš¨ê³¼ì ìœ¼ë¡œ ë¹„êµ ë¶„ì„í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆë‹¤.

1. denoising corrupted span

![Untitled](img/Untitled%202.png)

T5ì˜ pre-training objectiveëŠ” SpanBERTë…¼ë¬¸ì—ì„œ ì œì•ˆëœ ê¸°ë²•ì„ ì‚¬ìš©í•œë‹¤. t5ëŠ” SpanBERTì˜ token ë‹¨ìœ„ê°€ ì•„ë‹Œ span ë‹¨ìœ„ì˜ consecutive masking ê°œë…ë§Œ ì‚¬ìš©í•˜ì˜€ë‹¤. ****

1. ëŒ€ê·œëª¨ ì‚¬ì „ í›ˆë ¨ ë°ì´í„°ì…‹ (C4)

ì „ì´ í•™ìŠµì„ ìœ„í•œ ì¤‘ìš”í•œ ìš”ì†ŒëŠ” ì‚¬ì „ í›ˆë ¨ì— ì‚¬ìš©ë˜ëŠ” ë ˆì´ë¸”ì´ ì—†ëŠ” ë°ì´í„°ì…‹ì´ë‹¤.

wikipediaì˜ í…ìŠ¤íŠ¸ëŠ” í’ˆì§ˆì€ ìš°ìˆ˜í•˜ë‚˜ ìŠ¤íƒ€ì¼ì´ ê· ì¼í•˜ê³  ìƒëŒ€ì ìœ¼ë¡œ ì‘ìœ¼ë©°, Common crawl ì›¹ ìŠ¤í¬ë©ì€ í¬ê³  ë‹¤ì–‘í•˜ë‚˜ í’ˆì§ˆì´ ë‚®ë‹¤.

ì´ëŸ¬í•œ ìš”êµ¬ ì‚¬í•­ì„ ì¶©ì¡±ì‹œí‚¤ê¸° ìœ„í•´ wikipediaë³´ë‹¤ 2ë°° í° common crawl ë²„ì „ì¸ C4(Colossal Clean Crawled Corpus)ë¥¼ ê°œë°œ. ì—¬ê¸°ì— ì¤‘ë³µ ì œê±°, ë¶ˆì™„ì „í•œ ë¬¸ì¥ íê¸°, ë¶ˆí•„ìš”í•œ ë‚´ìš© ì œê±°ë¥¼ í†µí•´ ë°ì´í„° í•„í„°ë§ì„ í•˜ì˜€ë‹¤. ì´ëŸ¬í•œ í•„í„°ë§ì„ í†µí•´ overfitting ì—†ì´, ë” ë‚˜ì€ ê²°ê³¼ë¥¼ í†µí•´ ëª¨ë¸ í¬ê¸°ê°€ ì¦ê°€í•  ìˆ˜ ìˆì—ˆë‹¤. C4ëŠ” tensorflow datasetsì„ í†µí•´ ì œê³µëœë‹¤.

1. original encoder-decoder transformer

T5ì˜ model architectureëŠ” ê¸°ë³¸ Transformer êµ¬ì¡°ë¥¼ í¬ê²Œ ë²—ì–´ë‚˜ì§€ ì•ŠëŠ”ë‹¤. BERTë‚˜ GPT ê°™ì€ ëª¨ë¸ì²˜ëŸ¼ Transformer êµ¬ì¡°ì˜ Encoderë‚˜ Deocoderë¥¼ ë”°ë¡œ ë–¼ì–´ë‚´ì„œ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ê·¸ëƒ¥ ì›ë˜ Transformerì˜ Encoder-Decoder êµ¬ì¡°ë¥¼ ê·¸ëŒ€ë¡œ ê°€ì ¸ì™€ì„œ ì‚¬ìš©í•œë‹¤. ë‹¤ë§Œ ì•½ê°„ì˜ ë³€ê²½ì ì€ ìˆë‹¤.

- Transformerì˜ Layer Normalizationì— ì‚¬ìš©ë˜ëŠ” biasë¥¼ ì œê±°í•˜ê³  rescaleë§Œ ìˆ˜í–‰
- Absolute positional embedding ëŒ€ì‹  Relative positional embedding ì‚¬ìš©
- Model layer ì „ì²´ì—ì„œ position embedding parameter sharing

1. Multi-task pre-training

Multi-task learningì´ë€ í•˜ë‚˜ì˜ unsupervised taskì— ëŒ€í•´ì„œ pre-trainingì„ ì§„í–‰í•œ í›„ fine-tuningí•˜ëŠ” ê²ƒ ëŒ€ì‹ ì— ì—¬ëŸ¬ ì¢…ë¥˜ì˜ taskì— ëŒ€í•´ì„œ í•œ ë²ˆì— trainingì„ ì§„í–‰í•˜ëŠ” ê²ƒì„ ë§í•œë‹¤.

ë…¼ë¬¸ì—ì„œëŠ” ì´ multi-task learning ë°©ì‹ê³¼ pre-train + fine-tune ë°©ì‹ì˜ ì„±ëŠ¥ ë¹„êµë¥¼ ì§„í–‰í•˜ê³ ì í–ˆë‹¤. Multi-task learningì˜ ê²½ìš°ì—ëŠ” ê° taskë³„ data ì‚¬ìš© ë¹„ìœ¨ì— ë”°ë¼ ì„±ëŠ¥ì´ ë‹¬ë¼ì§€ê²Œ ë˜ëŠ”ë°, ìì¹« ë„ˆë¬´ ë§ì€ ì–‘ì˜ dataë¥¼ trainingì—ì„œ í™œìš©í•˜ê²Œ ëœë‹¤ë©´ training datasetì„ memorizeí•˜ê²Œ ëœë‹¤. ê·¸ë˜ì„œ ë…¼ë¬¸ì—ì„œëŠ” ì—¬ëŸ¬ê°€ì§€ ë¹„ìœ¨ ì„¤ì • ë°©ì‹ì— ëŒ€í•´ì„œ ë¹„êµë¥¼ ì§„í–‰í–ˆë‹¤.

ì—¬ëŸ¬ ì‹¤í—˜ë“¤ì„ ê±°ì¹œ í›„ T5 ëª¨ë¸ì´ ì„ íƒí•œ ê¸°ë²•ë“¤ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

- Span-corruption objective
- Longer pre-training (1 million steps on batch sizeÂ 211)
- Larger model (11B parameters)
- Multi-task pre-training + fine-tuning
- Beam Search

## reference

[https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html)

[https://velog.io/@mooncy0421/Paper-Review-T5-Exploring-the-Limits-of-Transfer-Learning-with-a-Unified-Text-to-Text-Transformer](https://velog.io/@mooncy0421/Paper-Review-T5-Exploring-the-Limits-of-Transfer-Learning-with-a-Unified-Text-to-Text-Transformer)

# 2. Installation

NAME="Linux Mint"
VERSION="20.2 (Uma)"

library version file: env/environment.yaml

# 3. Run